import string
import warnings

import numpy as np
import scipy
from matplotlib import pyplot as plt
from sklearn.cluster import KMeans
from sklearn.cluster import SpectralClustering, DBSCAN, OPTICS
from sklearn.metrics import silhouette_score

from comparaison_classes import CachedCustomLCS, Singleton
from segmentation import get_segmentation

warnings.simplefilter("ignore")
# TODO : better warnings

# TODO: link to where I picked some code (Spectral)
# TODO: documentation

DEBUG = True
GRAPHS = False


class PacketObject:
	"""
	Not used anymore because lists/standard python+numpy needs to be used for C implementation of Custom LCS
	Hash function to ensure it can be a dictionary key
	"""
	
	def __init__(self, seq_type, **kwargs):
		self.type = seq_type
		self.size = kwargs.get('size', None)
		self.tls_version = kwargs.get('tls_version', None)
	
	def __eq__(self, other):
		if not isinstance(other, PacketObject):
			return False
		return self.type == other.type and self.size == other.size and self.tls_version == other.tls_version
	
	def __hash__(self):
		return hash((self.type, self.size, self.tls_version))


class SegmentedConvertor(metaclass=Singleton):
	# Used to convert the segmented flows to numerical representation to allow computation of LCS
	# Single class instanciation to maintain a coherent type conversion (HANDSHAKE, etc)
	
	def __init__(self):
		self.type_to_int_dict = {}
	
	def segmented_to_typegroups(self, segmented_list):
		list_of_packetobjects = []
		if not self.type_to_int_dict:
			current_int = 0
		else:
			current_int = self.type_to_int_dict[max(self.type_to_int_dict, key=self.type_to_int_dict.get)] + 1
		for flow in segmented_list:
			flow_obj = []
			for sequence in flow:
				seq_type = sequence[0]
				if seq_type in self.type_to_int_dict:
					seq_type_int = self.type_to_int_dict[seq_type]
				else:
					seq_type_int = current_int
					self.type_to_int_dict[seq_type] = current_int
					current_int += 1
				pck_len = sequence[1][1] - sequence[1][0] + 1
				# time_len = sequence[2][1] - sequence[2][0]
				# IF type tls handshake : add tls metadata
				typegroup = [seq_type_int]
				if seq_type in ["TLS_HANDSHAKE", 'Failed_TLS_handshake']:
					typegroup.append(int(sequence[3]))
					typegroup.append(-2)
				elif seq_type in ["IN_TLS", "OUT_TLS"]:
					typegroup.append(int(sequence[4]))
					typegroup.append(int(sequence[3]))
				elif seq_type in ["IN", "OUT"]:
					typegroup.append(-2)
					typegroup.append(int(sequence[3]))
				else:
					typegroup.append(-2)
					typegroup.append(-2)
				flow_obj += tuple([tuple(typegroup) for _ in range(pck_len)])
			list_of_packetobjects.append(tuple(flow_obj))
		return tuple(list_of_packetobjects)
	
	def print_dict(self):
		return self.type_to_int_dict


def segmented_to_char(segmented_list):
	type_to_char_dict = {}
	current_letter = iter(string.ascii_lowercase)
	list_of_str = []
	for flow in segmented_list:
		str_from_seq = ""
		for sequence in flow:
			seq_type = sequence[0]
			pck_len = sequence[1][1] - sequence[1][0] + 1
			if seq_type in type_to_char_dict:
				str_from_seq += type_to_char_dict[seq_type] * pck_len
			else:
				letter = next(current_letter)
				type_to_char_dict[seq_type] = letter
				str_from_seq += letter * pck_len
		
		list_of_str.append(str_from_seq)
	
	return list_of_str, type_to_char_dict


def custom_dist(a, b):
	return a != b


def build_similarity_matrix(X):
	LCS_engine = CachedCustomLCS()
	X = np.array(X)
	n = X.shape[0]
	similarities = np.zeros((n, n))
	for i in range(n):
		for j in range(i):
			similarities[i][j] = LCS_engine.compute(X[i], X[j])
	similarity_matrix = (similarities + similarities.T)
	np.fill_diagonal(similarity_matrix, 1)
	return similarity_matrix


# Spectral
def build_laplacian(W, laplacian_normalization='rw'):
	"""
	Compute graph Laplacian.
	Parameters
	----------
	W : numpy array
		Adjacency matrix (n x n)
	laplacian_normalization : str
		String selecting which version of the laplacian matrix to construct.
			'unn':  unnormalized,
			'sym': symmetric normalization
			'rw':  random-walk normalization
	Returns
	-------
	L: (n x n) dimensional matrix representing the Laplacian of the graph
	"""
	L = np.zeros(W.shape)
	n = W.shape[0]
	D = np.diag(np.sum(W, axis=1))
	if laplacian_normalization == 'sym':
		L = np.identity(n) - 1 / np.sqrt(D) @ W @ 1 / np.sqrt(D)
	if laplacian_normalization == 'rw':
		L = np.identity(n) - (1 / (1 + D)) @ W
	else:
		L = D - W
	return L


def spectral_clustering_handmade(L, chosen_eig_indices=None, num_classes=None):
	"""
	Parameters
	----------
	L : numpy array
		Graph Laplacian (standard or normalized)
	chosen_eig_indices : list or None
		Indices of eigenvectors to use for clustering.
		If None, use adaptive choice of eigenvectors.
	num_classes : int
		Number of clusters to compute (defaults to 2)
	Returns
	-------
	Y : numpy array (num_samples, )
		Cluster assignments
	"""
	
	"""
	Use the function scipy.linalg.eig or the function scipy.sparse.linalg.eigs to compute:
	U = (n x n) eigenvector matrix           (sorted)
	E = (n x n) eigenvalue diagonal matrix   (sorted)
	"""
	E, U = scipy.linalg.eig(L)
	U = U.real
	U = U[:, E.argsort()]
	"""
	compute the clustering assignment from the eigenvectors
	Y = (n x 1) cluster assignments in [0,1,...,num_classes-1]
	"""
	# https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/
	optimal_k = num_classes
	if optimal_k is None:
		k_choices = range(2, 11)
		sum_of_squared_distances = []
		sii = []
		for k in k_choices:
			kmeans = KMeans(n_clusters=k, random_state=42)
			kmeans.fit(U[:, chosen_eig_indices])
			Y = kmeans.predict(U[:, chosen_eig_indices])
			label = kmeans.labels_
			sil_coeff = silhouette_score(U[:, chosen_eig_indices], label, metric='euclidean')
			sii.append(sil_coeff)
			sum_of_squared_distances.append(kmeans.inertia_)
		if GRAPHS:
			plt.plot(k_choices, sii)
			plt.show()
		# TODO: alternative to elbow/kmeans
		# FIXME: division by 0 etc
		derivative2 = np.gradient(sum_of_squared_distances, edge_order=1)
		derivative2 = derivative2 / derivative2[0]
		"""plt.plot(k_choices, derivative2)
		plt.show()"""
		_elbow = next(x for x, val in enumerate(derivative2) if val < 0.2)
		optimal_k = k_choices[_elbow - 1]
	print("Number of clusters : ", optimal_k)
	kmeans = KMeans(n_clusters=optimal_k, random_state=42)
	kmeans.fit(U[:, chosen_eig_indices])
	Y = kmeans.predict(U[:, chosen_eig_indices])
	return Y, optimal_k


def cluster_segmented_flow_old(segmented_flow, nb_class=None):
	segmented_convertor = SegmentedConvertor()
	stringlify_segmentation = segmented_convertor.segmented_to_typegroups(segmented_flow)
	sim_matrix = build_similarity_matrix(stringlify_segmentation)
	L = build_laplacian(sim_matrix)
	Y_rec, nb_class = spectral_clustering_handmade(L, chosen_eig_indices=[1], num_classes=nb_class)
	return Y_rec, nb_class


def cluster_segmented_flow(segmented_flow, nb_class=8, method="spectral"):
	segmented_convertor = SegmentedConvertor()
	stringlify_segmentation = segmented_convertor.segmented_to_typegroups(segmented_flow)
	sim_matrix = build_similarity_matrix(stringlify_segmentation)
	if method == "spectral":
		clustering = SpectralClustering(n_clusters=nb_class, affinity="precomputed", assign_labels="discretize",
		                                random_state=42).fit(sim_matrix)
		return clustering.labels_, nb_class
	elif method == "dbscan":
		dist_matrix = sim_matrix.copy()
		dist_matrix = 1 - dist_matrix
		clustering = DBSCAN(eps=1 - 0.3, metric="precomputed", n_jobs=-1).fit(dist_matrix)
		labels_raw = clustering.labels_
		labels_raw[labels_raw == -1] = max(labels_raw)
		return labels_raw, max(labels_raw) + 1
	elif method == "optics":
		dist_matrix = sim_matrix.copy()
		dist_matrix = 1 - dist_matrix
		clustering = OPTICS(metric="precomputed", n_jobs=-1).fit(dist_matrix)
		labels_raw = clustering.labels_
		labels_raw[labels_raw == -1] = max(labels_raw)
		return labels_raw, max(labels_raw) + 1


def visualize_clustering(cluster_indexes, segmentations):
	# TODO: import it
	color_dictionary = {
		'HANDSHAKE': 'xkcd:goldenrod',
		'TERM': 'xkcd:maroon',
		"IN": 'xkcd:green',
		'IN_TLS': 'xkcd:chartreuse',
		'OUT': 'xkcd:blue',
		'OUT_TLS': 'xkcd:sky blue',
		'TERM_RST': 'xkcd:red',
		"HANDSHAKE_FAILED": 'xkcd:purple',
		'TLS_handshake': 'tab:brown',
		'Failed_TLS_handshake': 'xkcd:crimson'
	}
	assert (len(cluster_indexes) == len(segmentations))
	n_classes = max(cluster_indexes) + 1
	plot_max_y_indexes = [0 for _ in range(n_classes)]
	for i in range(len(segmentations)):
		segmentation = segmentations[i]
		cluster_index = cluster_indexes[i] + 1
		plt.subplot(n_classes + 1, 1, cluster_index)
		for j in range(len(segmentation)):
			color = color_dictionary.get(segmentation[j][0], 'tab:orange')
			plt.broken_barh([(int(segmentation[j][1][0]), 1 + int(segmentation[j][1][1]) - int(segmentation[j][1][0]))],
			                (plot_max_y_indexes[cluster_index - 1], 1), facecolors=color, label=segmentation[j][0])
			plt.xticks([], [])
		plot_max_y_indexes[cluster_index - 1] += 1
	# Display labels
	fig = plt.gcf()
	st = fig.suptitle("Clustering visualization", fontsize="x-large")
	lines_labels = [ax.get_legend_handles_labels() for ax in fig.axes]
	ax2 = fig.add_subplot(n_classes + 1, 1, n_classes + 1)
	ax2.axis("off")
	label_dict = {}
	for elem in lines_labels:
		by_label = dict(zip(elem[1], elem[0]))
		label_dict.update(by_label)
	bbox = ax2.get_window_extent().transformed(fig.dpi_scale_trans.inverted())
	ax2.legend(label_dict.values(), label_dict.keys(), loc="center", ncol=3, bbox_to_anchor=(0.5, 0))
	plt.show()


def evaluate_clustering(segmentations):
	segmented_convertor = SegmentedConvertor()
	stringlify_segmentation = segmented_convertor.segmented_to_typegroups(segmentations)
	sim_matrix = build_similarity_matrix(stringlify_segmentation)
	dist_matrix = sim_matrix.copy()
	dist_matrix = 1 - dist_matrix
	scores = []
	range_nb_cluster = range(2, 30)
	for i in range_nb_cluster:
		clustering = SpectralClustering(n_clusters=i, affinity="precomputed", assign_labels="discretize",
		                                random_state=42).fit(sim_matrix)
		scores.append(silhouette_score(dist_matrix, clustering.labels_, metric="precomputed"))
	plt.plot(range_nb_cluster, scores)
	plt.show()


if __name__ == "__main__":
	segmentations, _ = get_segmentation(path="./trickbot1_2/")
	cluster_indexes, nb_class = cluster_segmented_flow(segmentations, 6)
	evaluate_clustering(segmentations)
	# CachedCustomLCS().print_stat()
	visualize_clustering(cluster_indexes, segmentations)
