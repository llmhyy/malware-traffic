import numpy as np
from matplotlib import pyplot as plt
import scipy
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
import string
from comparaison_classes import CachedDTW, CachedLCS, CachedCustomLCS, Singleton
from segmentation import get_segmentation
import warnings

warnings.simplefilter("ignore")
# TODO : better warnings

# TODO: link to where I picked some code (Spectral + hierarchical)
# TODO: documentation

DEBUG = True

class PacketObject:
	# Not used anymore
	def __init__(self, seq_type, **kwargs):
		self.type = seq_type
		self.size = kwargs.get('size', None)
		self.tls_version = kwargs.get('tls_version', None)
	
	def __eq__(self, other):
		if not isinstance(other, PacketObject):
			return False
		return self.type == other.type and self.size == other.size and self.tls_version == other.tls_version
	
	def __hash__(self):
		return hash((self.type, self.size, self.tls_version))


class SegmentedConvertor(metaclass=Singleton):
	# Used to convert the segmented flows to numerical representation to allow computation of LCS
	# Single class instanciation to maintain a coherent type conversion (HANDSHAKE, etc)

	def __init__(self):
		self.type_to_int_dict = {}
	
	def segmented_to_typegroups(self, segmented_list):
		list_of_packetobjects = []
		if not self.type_to_int_dict:
			current_int = 0
		else:
			current_int = self.type_to_int_dict[max(self.type_to_int_dict, key=self.type_to_int_dict.get)] + 1
		for flow in segmented_list:
			flow_obj = []
			for sequence in flow:
				seq_type = sequence[0]
				if seq_type in self.type_to_int_dict:
					seq_type_int = self.type_to_int_dict[seq_type]
				else:
					seq_type_int = current_int
					self.type_to_int_dict[seq_type] = current_int
					current_int += 1
				pck_len = sequence[1][1] - sequence[1][0] + 1
				# time_len = sequence[2][1] - sequence[2][0]
				# IF type tls handshake : add tls metadata
				typegroup = [seq_type_int]
				if seq_type in ["TLS_HANDSHAKE", 'Failed_TLS_handshake']:
					typegroup.append(int(sequence[3]))
					typegroup.append(-2)
				elif seq_type in ["IN_TLS", "OUT_TLS"]:
					typegroup.append(int(sequence[4]))
					typegroup.append(int(sequence[3]))
				elif seq_type in ["IN", "OUT"]:
					typegroup.append(-2)
					typegroup.append(int(sequence[3]))
				else:
					typegroup.append(-2)
					typegroup.append(-2)
				flow_obj += tuple([tuple(typegroup) for _ in range(pck_len)])
			list_of_packetobjects.append(tuple(flow_obj))
		return tuple(list_of_packetobjects)
	
	def print_dict(self):
		return self.type_to_int_dict


def segmented_to_char(segmented_list):
	type_to_char_dict = {}
	current_letter = iter(string.ascii_lowercase)
	list_of_str = []
	for flow in segmented_list:
		str_from_seq = ""
		for sequence in flow:
			seq_type = sequence[0]
			pck_len = sequence[1][1] - sequence[1][0] + 1
			if seq_type in type_to_char_dict:
				str_from_seq += type_to_char_dict[seq_type] * pck_len
			else:
				letter = next(current_letter)
				type_to_char_dict[seq_type] = letter
				str_from_seq += letter * pck_len
		
		list_of_str.append(str_from_seq)
	
	return list_of_str, type_to_char_dict


def custom_dist(a, b):
	return a != b


def build_similarity_matrix(X):
	LCS_engine = CachedCustomLCS()
	X = np.array(X)
	n = X.shape[0]
	similarities = np.zeros((n, n))
	for i in range(n):
		for j in range(i):
			similarities[i][j] = LCS_engine.compute(X[i], X[j])
	return similarities + similarities.T - np.diag(similarities.diagonal())


# Different types of clustering
# Spectral clustering - Hierarchical

# Spectral
def build_laplacian(W, laplacian_normalization='unn'):
	"""
	Compute graph Laplacian.

	Parameters
	----------
	W : numpy array
		Adjacency matrix (n x n)
	laplacian_normalization : str
		String selecting which version of the laplacian matrix to construct.
			'unn':  unnormalized,
			'sym': symmetric normalization
			'rw':  random-walk normalization

	Returns
	-------
	L: (n x n) dimensional matrix representing the Laplacian of the graph
	"""
	L = np.zeros(W.shape)
	n = W.shape[0]
	D = np.diag(np.sum(W, axis=1))
	if laplacian_normalization == 'sym':
		L = np.identity(n) - 1 / np.sqrt(D) @ W @ 1 / np.sqrt(D)
	if laplacian_normalization == 'rw':
		L = np.identity(n) - (1 / D) @ W
	else:
		L = D - W
	return L


def spectral_clustering(L, chosen_eig_indices=None, num_classes=None):
	"""
	Parameters
	----------
	L : numpy array
		Graph Laplacian (standard or normalized)
	chosen_eig_indices : list or None
		Indices of eigenvectors to use for clustering.
		If None, use adaptive choice of eigenvectors.
	num_classes : int
		Number of clusters to compute (defaults to 2)


	Returns
	-------
	Y : numpy array (num_samples, )
		Cluster assignments
	"""
	
	"""
	Use the function scipy.linalg.eig or the function scipy.sparse.linalg.eigs to compute:
	U = (n x n) eigenvector matrix           (sorted)
	E = (n x n) eigenvalue diagonal matrix   (sorted)
	"""
	E, U = scipy.linalg.eig(L)
	U = U.real
	U = U[:, E.argsort()]
	"""
	compute the clustering assignment from the eigenvectors
	Y = (n x 1) cluster assignments in [0,1,...,num_classes-1]
	"""
	# https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/
	optimal_k = num_classes
	if optimal_k is None:
		k_choices = range(2, 11)
		sum_of_squared_distances = []
		sii = []
		for k in k_choices:
			kmeans = KMeans(n_clusters=k, random_state=42)
			kmeans.fit(U[:, chosen_eig_indices])
			Y = kmeans.predict(U[:, chosen_eig_indices])
			label = kmeans.labels_
			sil_coeff = silhouette_score(U[:, chosen_eig_indices], label, metric='euclidean')
			sii.append(sil_coeff)
			sum_of_squared_distances.append(kmeans.inertia_)
		plt.show()
		plt.plot(k_choices, sii)
		plt.show()
		# TODO: alternative to elbow/kmeans
		# FIXME: division by 0 etc
		derivative2 = np.gradient(sum_of_squared_distances, edge_order=2)
		derivative2 = derivative2 / derivative2[0]
		"""plt.plot(k_choices, derivative2)
		plt.show()"""
		derivative2 = np.gradient(sum_of_squared_distances, edge_order=1)
		derivative2 = derivative2 / derivative2[0]
		"""plt.plot(k_choices, derivative2)
		plt.show()"""
		_elbow = next(x for x, val in enumerate(derivative2) if val < 0.2)
		optimal_k = k_choices[_elbow - 1]
	print("Number of clusters : ", optimal_k)
	kmeans = KMeans(n_clusters=optimal_k, random_state=42)
	kmeans.fit(U[:, chosen_eig_indices])
	Y = kmeans.predict(U[:, chosen_eig_indices])
	return Y, optimal_k


def cluster_segmented_flow(segmented_flow, nb_class=None):
	segmented_convertor = SegmentedConvertor()
	stringlify_segmentation = segmented_convertor.segmented_to_typegroups(segmented_flow)
	sim_matrix = build_similarity_matrix(stringlify_segmentation)
	L = build_laplacian(sim_matrix)
	Y_rec, nb_class = spectral_clustering(L, chosen_eig_indices=[1], num_classes=nb_class)
	return Y_rec, nb_class


if __name__ == "__main__":
	segmentations, _ = get_segmentation(path="./trickbot1_1/")
	cluster_indexes, nb_class = cluster_segmented_flow(segmentations)
	print(nb_class)
	a = CachedCustomLCS()
	a.print_stat()

