import inspect
import tensorflow as tf
import numpy as np
import pprint
import hashlib
import re
from sklearn.metrics import silhouette_score
from sklearn.cluster import SpectralClustering
import matplotlib.pyplot as plt
import os
from api_extraction import MalwareTraceExtractor
from string import ascii_lowercase
from itertools import product
from gensim.models import Word2Vec
import multiprocessing
import seaborn as sns;

sns.set_theme()

import logging  # Setting up the loggings to monitor gensim

logging.basicConfig(format="%(levelname)s - %(asctime)s: %(message)s", datefmt='%H:%M:%S', level=logging.INFO)


class Singleton(type):
    # Singleton modified to handle arguments (singleton for each argument set)
    _instances = {}
    _init = {}
    
    def __init__(cls, name, bases, dct):
        cls._init[cls] = dct.get('__init__', None)
    
    def __call__(cls, *args, **kwargs):
        init = cls._init[cls]
        if init is not None:
            key = (cls, frozenset(inspect.getcallargs(init, None, *args, **kwargs).items()))
        else:
            key = cls
        
        if key not in cls._instances:
            cls._instances[key] = super(Singleton, cls).__call__(*args, **kwargs)
        return cls._instances[key]


class ApiEmbedding(metaclass=Singleton):
    def __init__(self, malware_tuple, force=False):
        self.tuple_hash = hashlib.md5(repr(malware_tuple).encode()).hexdigest()
        # Extract API calls
        self.record = []
        for malware in malware_tuple:
            a = MalwareTraceExtractor("malware.exe", malware)
            b = a.get_merge_trace()
            self.record.append(list(b[:, 1]))
        self.unique_words = list(set([item for sublist in self.record for item in sublist]))
        self.uw_count = len(self.unique_words)
        # Check if old weight exists
        self.malware_list_hash = hash(malware_tuple)
        if not os.path.exists(f"w2v_weigth_{self.tuple_hash[:8]}.model") or force:
            # Train model
            cores = multiprocessing.cpu_count()
            self.model = Word2Vec(min_count=0,
                                  window=6,
                                  vector_size=256,
                                  alpha=0.03,
                                  min_alpha=0.001,
                                  negative=5,
                                  workers=cores - 1)
            self.model.build_vocab(self.record, progress_per=10000)
            self.model.train(self.record, total_examples=self.model.corpus_count, epochs=2000, report_delay=2)
            self.model.init_sims(replace=True)
            print(self.model.wv.most_similar(positive=["LocalAlloc"]))
            self.model.save(f"w2v_weigth_{self.tuple_hash[:8]}.model")
        else:
            # Load model
            self.model = Word2Vec.load(f"w2v_weigth_{self.tuple_hash[:8]}.model")
    
    def compute_sim(self, x, y):
        a = self.model.wv.similarity(x, y)
        return max(a, 0)
    
    def generate_sim_matrix(self):
        sim_matrix = np.zeros((self.uw_count, self.uw_count))
        for i in range(self.uw_count):
            for j in range(i):
                sim_matrix[i, j] = self.compute_sim(self.unique_words[i], self.unique_words[j])
        sim_matrix = sim_matrix + sim_matrix.T
        for i in range(self.uw_count):
            sim_matrix[i, i] = 1
        print(np.min(sim_matrix), np.max(sim_matrix))
        return sim_matrix
    
    def generate_dist_matrix(self):
        sim_matrix = self.generate_sim_matrix()
        dist_matrix = 1 - sim_matrix
        return dist_matrix
    
    def visualize_matrix(self):
        sim_matrix = self.generate_sim_matrix()
        sns.heatmap(sim_matrix)
    
    def test_cluster(self):
        sim_matrix = self.generate_sim_matrix()
        dist_matrix = self.generate_dist_matrix()
        range_nb_cluster = range(2, 64)
        scores = []
        for i in range_nb_cluster:
            clustering = SpectralClustering(n_clusters=i, affinity="precomputed", assign_labels="discretize",
                                            random_state=42, n_jobs=-1).fit(sim_matrix)
            score = silhouette_score(dist_matrix, clustering.labels_, metric="precomputed")
            scores.append(score)
        
        plt.plot(range_nb_cluster, scores)
        plt.show()
    
    def cluster(self, nb_cluster):
        sim_matrix = self.generate_sim_matrix()
        dist_matrix = self.generate_dist_matrix()
        clustering = SpectralClustering(n_clusters=nb_cluster, affinity="precomputed", assign_labels="discretize",
                                        random_state=42, n_jobs=-1).fit(sim_matrix)
        return clustering.labels_

    def get_api_with_cluster(self, clustering_labels, cluster):
        return np.array(self.unique_words)[clustering_labels == cluster]
    
    
if __name__ == "__main__":
    malware_list = ("trickbot1_1/", "trickbot1_2/", "trickbot1_3/", "trickbot1_4/", "trickbot1_5/")
    a = ApiEmbedding(malware_list, force=True)
    #a.test_cluster()
    b = a.cluster(48)
    for i in range(48):
        print(i, a.get_api_with_cluster(b, i))
