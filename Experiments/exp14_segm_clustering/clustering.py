import numpy as np
from fastdtw import fastdtw
from matplotlib import pyplot as plt
import scipy
import scipy.cluster as cluster
from sklearn.cluster import KMeans

#TODO: link to where i picked some code

DEBUG = True
a, b = 'AAAAAAAAAAAAAAAA', 'AAAAAAA'
l1, l2 = len(a), len(b)
list_of_str = ['a', 'aaaaa', 'aaaaa', 'aaaaa', 'bcd', 'a', 'a', 'aaaaa', 'aaaaa', 'aaaaa', 'befgfgfgfgfgfgfgfgfgfgfgfgfgfgfgfgfgfgfghfcdd', 'bchc', 'bechcccchhccdd', 'befgfgfgfgfgfgfgfgfgfgfgfgfghfcdd', 'bchi', 'befghfcdd', 'befgfchcd', 'befghfcdd', 'befghfcdd', 'bef']
list_of_str = ['a', 'aaaaa', 'aaaaa', 'aaaaa', 'a', 'bcd', 'aaaaa', 'aaaaa', 'aaaaa', 'a', 'aaaaa', 'aaaaa', 'aaaaa', 'befgfgfgfgfghfcdd', 'begfgfch', 'a', 'a', 'a', 'a', 'bechchchchchchchchchcccchchcccchcchchchcccccchccccchcccchcccchccccccchcchchchcchchchccccchccchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchcchchchcchchchcccchchcccchcccchcccccchccchccchchcchchchccchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchchccchchcchchchccchcchcddd', 'befgfgfgfgfchcd', 'befgfchcd', 'bchi', 'becfghfcdd', 'bc', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'bchi', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'a', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'a', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'a', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'a', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'a', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'a', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'a', 'a', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'a', 'aaaaa', 'aaaaa', 'a', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'a', 'aaaaa', 'aaaaa', 'aaaaa', 'a', 'aaaaa', 'aaaaa', 'a', 'aaaaa', 'aaaaa', 'aaaaa', 'a', 'aaaaa', 'a', 'aaaaa', 'aaaaa', 'aaaaa', 'aaaaa', 'a', 'aaaaa', 'aaaaa', 'a', 'aaaaa', 'a', 'a', 'aaaaa', 'a', 'a', 'a', 'a', 'bef', 'a', 'a', 'a', 'a']

def custom_dist(a,b):
    return a!=b

def distance(a, b):
    l1, l2 = len(a), len(b)
    a = [ord(i) for i in a]
    b = [ord(i) for i in b]

    dtw_distance, _ = fastdtw(list(a), list(b), dist=custom_dist)
    sim = (max(l1, l2) - dtw_distance) / max(l1, l2)
    return sim

def build_similarity_matrix(X):
    X = np.array(X)
    n = X.shape[0]
    similarities = np.zeros((n, n))
    for i in range(n):
        for j in range(i):
            similarities[i][j] = distance(X[i], X[j])
    return similarities + similarities.T - np.diag(similarities.diagonal())

sim = build_similarity_matrix(list_of_str)
plt.imshow(sim, interpolation='nearest')
plt.show()

#Different types of clustering
#Spectral clustering - Hierarchical

#Spectral
def build_laplacian(W, laplacian_normalization='unn'):
    """
    Compute graph Laplacian.

    Parameters
    ----------
    W : numpy array
        Adjacency matrix (n x n)
    laplacian_normalization : str
        String selecting which version of the laplacian matrix to construct.
            'unn':  unnormalized,
            'sym': symmetric normalization
            'rw':  random-walk normalization   

    Returns
    -------
    L: (n x n) dimensional matrix representing the Laplacian of the graph
    """
    L = np.zeros(W.shape)
    n = W.shape[0]
    D = np.diag( np.sum( W , axis = 1 ) ) 
    if laplacian_normalization == 'sym':
        L = np.identity(n) - 1/np.sqrt(D) @ W @ 1/np.sqrt(D)
    if laplacian_normalization == 'rw':
        L = np.identity(n) - (1/D) @ W
    else : 
        L = D - W
    return L

def spectral_clustering(L, chosen_eig_indices=None, num_classes=2):
    """
    Parameters
    ----------
    L : numpy array
        Graph Laplacian (standard or normalized)
    choosen_eig_indices : list or None
        Indices of eigenvectors to use for clustering. 
        If None, use adaptive choice of eigenvectors.
    num_classes : int 
        Number of clusters to compute (defaults to 2)


    Returns
    -------
    Y : numpy array (num_samples, )
        Cluster assignments
    """

    """
    Use the function scipy.linalg.eig or the function scipy.sparse.linalg.eigs to compute:
    U = (n x n) eigenvector matrix           (sorted)
    E = (n x n) eigenvalue diagonal matrix   (sorted)
    """
    E , U = scipy.linalg.eig(L)
    U = U.real
    U = U[:,E.argsort()]
    """
    compute the clustering assignment from the eigenvectors        
    Y = (n x 1) cluster assignments in [0,1,...,num_classes-1]                   
    """
    if DEBUG:
        k_choices = range(2, 10)
        Sum_of_squared_distances = []
        for k in k_choices:
            kmeans = KMeans(n_clusters = k)
            kmeans.fit(U[:,chosen_eig_indices])
            Y = kmeans.predict(U[:,chosen_eig_indices])
            Sum_of_squared_distances.append(kmeans.inertia_)
        plt.plot(k_choices, Sum_of_squared_distances, 'bx-')
        plt.xlabel('k')
        plt.ylabel('Sum_of_squared_distances')
        plt.title('Elbow Method For Optimal k')
        plt.show()
    else:
        kmeans = KMeans(n_clusters = num_classes)
        kmeans.fit(U[:,chosen_eig_indices])
        Y = kmeans.predict(U[:,chosen_eig_indices])
    return Y

num_classes = 3
L = build_laplacian(sim)
Y_rec = spectral_clustering(L, chosen_eig_indices=[1], num_classes=num_classes)

# Plot results, comparing to KMeans
print(Y_rec)

# Hierarchical

import scipy.spatial.distance as ssd
# convert the redundant n*n square matrix form into a condensed nC2 array
distArray = ssd.squareform(sim) # distArray[{n choose 2}-{n-i choose 2} + (j-i-1)] is the distance between points i and j
Z = (cluster.hierarchy.linkage(sim, method='ward', metric='euclidean'))
plt.title('Hierarchical Clustering Dendrogram (truncated)')
plt.xlabel('sample index')
plt.ylabel('distance')
cluster.hierarchy.dendrogram(
    Z,
    truncate_mode='lastp',  # show only the last p merged clusters
    p=12,  # show only the last p merged clusters
    show_leaf_counts=False,  # otherwise numbers in brackets are counts
    leaf_rotation=90.,
    leaf_font_size=12.,
    show_contracted=True,  # to get a distribution impression in truncated branches
)
plt.show()
